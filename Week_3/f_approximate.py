# -*- coding: utf-8 -*-
"""F_approximate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I2l1Rem950KCM3b9fiX8WYJlew_SWQpP
"""

# Neural Network for Runge Function Approximation
# 使用神經網路逼近 Runge 函數 f(x) = 1/(1+25x^2)

import math
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt


# =============================================================================
# 1. Target Function and Derivative
# =============================================================================

def runge(x: torch.Tensor) -> torch.Tensor:
    return 1.0 / (1.0 + 25.0 * x**2)

# =============================================================================
# 2. Sampling Functions
# =============================================================================

def sample_uniform(n: int, device: str = "cpu") -> torch.Tensor:
    return 2.0 * torch.rand(n, 1, device=device) - 1.0


def sample_chebyshev(n: int, device: str = "cpu") -> torch.Tensor:
    """
    Chebyshev-like sampling: denser near endpoints ±1
    使樣本能更密集分布在端點，以減少Runge function在端點震盪所造成的近似誤差
    """
    u = torch.rand(n, 1, device=device)
    x = torch.cos(math.pi * u)
    return x


def chebyshev_weight(x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:
    """
    Chebyshev weighting function: w(x) = 1 / sqrt(1 - x^2 + eps)
    決定誤差加權（weighting）的方式，用來強化端點的影響力
    """
    w = 1.0 / torch.sqrt(torch.clamp(1.0 - x**2, min=0.0) + eps)
    return w / w.mean()


# =============================================================================
# 3. Neural Network Model
# =============================================================================

class MLP2(nn.Module):
    """
    Two-hidden-layer MLP with tanh activation
    Architecture: 1 -> width -> width -> 1
    使用兩層hidden layer，每層神經元數量為64(深度為2，寬度為64): (1-64-64-1)
    """

    def __init__(self, width: int = 64):
        super().__init__()
        self.l1 = nn.Linear(1, width)
        self.l2 = nn.Linear(width, width)
        self.l3 = nn.Linear(width, 1)
        self.reset_parameters_tanh()

    def reset_parameters_tanh(self):
        """
        Xavier/Glorot-uniform initialization with tanh gain
        初始化神經網路層參數，gain: 計算tanh activation function 對應的增益值，用來修正方差
        以 Xavier/Glorot-uniform 方法初始化權重，確保前向與反向訊號的方差平衡
        """
        gain = nn.init.calculate_gain('tanh')  # ≈ 5/3
        for layer in [self.l1, self.l2, self.l3]:
            nn.init.xavier_uniform_(layer.weight, gain=gain)
            nn.init.zeros_(layer.bias)  # 將bias設為0，避免初始偏移

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        h1 = torch.tanh(self.l1(x))
        h2 = torch.tanh(self.l2(h1))
        y = self.l3(h2)  # Linear output (no activation)
        return y


# =============================================================================
# 4. Evaluation Functions
# =============================================================================

@torch.no_grad()
def eval_metrics(model: nn.Module, xs: torch.Tensor) -> dict:
    """
    Compute RMSE and L∞ error metrics
    計算最大絕對誤差、RMSE(均方根誤差)
    """
    y_true = runge(xs)
    y_pred = model(xs)
    err = (y_pred - y_true).abs()
    mse = (err.pow(2).mean()).item()
    rmse = math.sqrt(mse)
    linf = err.max().item()
    return {"MSE": mse, "RMSE": rmse, "L_inf": linf}

# =============================================================================
# 5. Training Function
# =============================================================================

def train(
    steps: int = 3000,              # 訓練總步數
    batch_size: int = 512,          # 每步使用的樣本數
    width: int = 64,                # 網路寬度
    alpha: float = 0.7,             # 未加權MSE係數
    beta: float = 0.3,              # 端點加權MSE係數
    use_chebyshev: bool = False,    # 是否使用Chebyshev採樣
    use_sobolev: bool = False,      # 是否使用Sobolev正則化
    lambda_d: float = 0.1,          # 導數損失權重
    lr: float = 1e-3,               # 學習率
    weight_decay: float = 1e-6,     # 權重衰減
    device: str = "cpu",            # 計算設備
    seed: int = 0,                  # 隨機種子
    verbose: bool = True            # 是否顯示訓練過程
):

    # 設定隨機種子
    torch.manual_seed(seed)

    # 初始化模型和優化器
    model = MLP2(width=width).to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)

    # 訓練記錄
    loss_history = []
    evaluation_grid = torch.linspace(-1.0, 1.0, 2000, device=device).unsqueeze(1)

    # 選擇採樣函數
    sample_fn = sample_chebyshev if use_chebyshev else sample_uniform

    print(f"Starting training with {steps} steps...")
    print(f"Sampling: {'Chebyshev' if use_chebyshev else 'Uniform'}")
    print(f"Sobolev regularization: {use_sobolev}")
    print("-" * 60)

    for step in range(1, steps + 1):
        # 採樣訓練數據
        x = sample_fn(batch_size, device=device)
        if use_sobolev:
            x.requires_grad_(True)

        y_true = runge(x)

        # 前向傳播
        y_pred = model(x)
        error = y_pred - y_true

        # 計算基礎損失
        if beta > 0.0 and use_chebyshev:
            # 使用加權MSE
            w = chebyshev_weight(x)
            mse = (error.pow(2)).mean()
            wmse = ((w * error).pow(2)).mean()
            loss = alpha * mse + beta * wmse
        else:
            # 純MSE損失
            loss = (error.pow(2)).mean()

        # 反向傳播和參數更新
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        loss_history.append(loss.item())

        # 定期評估和輸出
        if verbose and (step % 500 == 0 or step == 1 or step == steps):
            with torch.no_grad():
                metrics = eval_metrics(model, evaluation_grid)
            print(f"Step {step:5d} | Loss: {loss.item():.4e} | "
                  f"RMSE: {metrics['RMSE']:.4e} | L∞: {metrics['L_inf']:.4e}")

    # 最終評估
    print("\n" + "="*60)
    print("FINAL EVALUATION")
    print("="*60)

    final_grid = torch.linspace(-1, 1, 5000, device=device).unsqueeze(1)
    final_metrics = eval_metrics(model, final_grid)

    print(f"Function approximation:")
    print(f"  RMSE  = {final_metrics['RMSE']:.6e}")
    print(f"  L_inf = {final_metrics['L_inf']:.6e}")

    # 可視化結果
    if verbose:
        visualize_results(model, loss_history, device)

    return model


# =============================================================================
# 6. Visualization Functions
# =============================================================================

def visualize_results(model: nn.Module, loss_history: list, device: str = "cpu"):
    """
    Visualize training results with multiple plots
    """
    # 準備數據
    xs = torch.linspace(-1.0, 1.0, 2000, device=device).unsqueeze(1)
    xs_grad = xs.clone().requires_grad_(True)

    with torch.no_grad():
        y_true = runge(xs)
        y_pred = model(xs)
        error = (y_pred - y_true).abs()


    # 轉換為numpy
    xs_np = xs.squeeze().cpu().numpy()
    y_true_np = y_true.squeeze().cpu().numpy()
    y_pred_np = y_pred.squeeze().cpu().numpy()
    error_np = error.squeeze().cpu().numpy()

    # 創建子圖
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle('Neural Network Approximation of Runge Function', fontsize=16)

    # 1. 函數比較
    axes[0, 0].plot(xs_np, y_true_np, 'b-', label='True Runge f(x)', linewidth=2)
    axes[0, 0].plot(xs_np, y_pred_np, 'r--', label='NN approximation', linewidth=2)
    axes[0, 0].set_title('Function Approximation')
    axes[0, 0].set_xlabel('x')
    axes[0, 0].set_ylabel('f(x)')
    axes[0, 0].grid(True, alpha=0.3)
    axes[0, 0].legend()

    # 2. 函數絕對誤差
    axes[0, 1].plot(xs_np, error_np, 'g-', linewidth=2)
    axes[0, 1].set_title('Function Absolute Error |f̂(x) - f(x)|')
    axes[0, 1].set_xlabel('x')
    axes[0, 1].set_ylabel('Absolute Error')
    axes[0, 1].grid(True, alpha=0.3)
    axes[0, 1].set_yscale('log')


    # 訓練損失曲線
    plt.figure(figsize=(10, 6))
    plt.plot(range(1, len(loss_history) + 1), loss_history, 'b-', linewidth=2)
    plt.title('Training Loss History')
    plt.xlabel('Training Step')
    plt.ylabel('Loss')
    plt.grid(True, alpha=0.3)
    plt.yscale('log')
    plt.show()


# =============================================================================
# 7. Main Execution
# =============================================================================

if __name__ == "__main__":
    print("Neural Network Approximation of Runge Function")
    print("=" * 60)

    # 訓練配置
    config = {
        'steps': 2000,
        'batch_size': 512,
        'width': 64,
        'alpha': 1.0,
        'beta': 0.0,           # beta=0 移除端點權重
        'use_chebyshev': False, # 使用uniform採樣
        'use_sobolev': False,   # 不使用Sobolev正則化
        'lambda_d': 0.1,
        'lr': 1e-3,
        'weight_decay': 1e-6,
        'device': 'cpu',
        'seed': 0,
        'verbose': True
    }

    # 開始訓練
    model = train(**config)

    print("\nTraining completed successfully!")