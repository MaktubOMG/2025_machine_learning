# -*- coding: utf-8 -*-
"""dF_approximate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eiEE2cJY_VcVG7baUe5fGbWE3872vees
"""

# Neural Network for Runge Derivative Approximation (pure-derivative training)
# 只學 f'(x)：訓練損失為導數 MSE；列印與圖表皆為導數版本

import math
import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt

# =========================
# 1) Target derivative
# =========================
def runge_dx(x: torch.Tensor) -> torch.Tensor:
    # f'(x) = -50 x / (1 + 25 x^2)^2
    return -50.0 * x / (1.0 + 25.0 * x**2)**2

# =========================
# 2) Sampling
# =========================
def sample_uniform(n: int, device: str = "cpu") -> torch.Tensor:
    return 2.0 * torch.rand(n, 1, device=device) - 1.0

def sample_chebyshev(n: int, device: str = "cpu") -> torch.Tensor:
    u = torch.rand(n, 1, device=device)   # U(0,1)
    return torch.cos(math.pi * u)         # denser near ±1

# =========================
# 3) Model
# =========================
class MLP2(nn.Module):
    """1 → width → width → 1 with tanh; linear output."""
    def __init__(self, width: int = 64):
        super().__init__()
        self.l1 = nn.Linear(1, width)
        self.l2 = nn.Linear(width, width)
        self.l3 = nn.Linear(width, 1)
        self.reset_parameters_tanh()

    def reset_parameters_tanh(self):
        gain = nn.init.calculate_gain('tanh')  # ≈ 5/3
        for layer in (self.l1, self.l2, self.l3):
            nn.init.xavier_uniform_(layer.weight, gain=gain)
            nn.init.zeros_(layer.bias)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        h1 = torch.tanh(self.l1(x))
        h2 = torch.tanh(self.l2(h1))
        return self.l3(h2)  # linear

# =========================
# 4) Derivative metrics
# =========================
def eval_derivative_metrics(model: nn.Module, n_points: int = 2000, device: str = "cpu") -> dict:
    xs = torch.linspace(-1.0, 1.0, n_points, device=device).unsqueeze(1)
    xs.requires_grad_(True)
    yhat = model(xs)
    dhat_dx = torch.autograd.grad(yhat.sum(), xs, create_graph=False)[0]
    dtrue_dx = runge_dx(xs)
    derr = (dhat_dx - dtrue_dx).abs()
    mse_d  = (derr.pow(2).mean()).item()
    rmse_d = math.sqrt(mse_d)
    linf_d = derr.max().item()
    return {"RMSE_d": rmse_d, "L_inf_d": linf_d}

# =========================
# 5) Training (loss = derivative MSE)
# =========================
def train(
    steps: int = 2000,
    batch_size: int = 512,
    width: int = 64,
    use_chebyshev: bool = False,
    lr: float = 1e-3,
    weight_decay: float = 1e-6,
    device: str = "cpu",
    seed: int = 0,
    verbose: bool = True,
    visualize: bool = False,
):
    torch.manual_seed(seed)
    model = MLP2(width=width).to(device)
    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    loss_hist = []
    sample_fn = sample_chebyshev if use_chebyshev else sample_uniform

    print("Neural Network Approximation of Runge Function (Derivative-only)")
    print("=" * 60)
    print(f"Starting training with {steps} steps...")
    print(f"Sampling: {'Chebyshev' if use_chebyshev else 'Uniform'}")
    print("Loss: derivative MSE (Sobolev style)")
    print("-" * 60)

    for t in range(1, steps + 1):
        x = sample_fn(batch_size, device=device)
        x.requires_grad_(True)                # 需要對 x 求導
        # forward
        yhat = model(x)
        dhat = torch.autograd.grad(yhat.sum(), x, create_graph=True)[0]  # create_graph=True 才能反傳到參數
        dtrue = runge_dx(x)
        # loss = MSE on derivatives
        loss = (dhat - dtrue).pow(2).mean()

        opt.zero_grad(set_to_none=True)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        opt.step()
        loss_hist.append(loss.item())

        if verbose and (t % 500 == 0 or t == 1 or t == steps):
            m = eval_derivative_metrics(model, n_points=2000, device=device)
            print(f"Step {t:5d} | Loss: {loss.item():.4e} | RMSE_d: {m['RMSE_d']:.4e} | L∞_d: {m['L_inf_d']:.4e}")

    print("\n" + "=" * 60)
    print("FINAL EVALUATION")
    print("=" * 60)
    final_m = eval_derivative_metrics(model, n_points=5000, device=device)
    print("Derivative approximation:")
    print(f"  RMSE_d  = {final_m['RMSE_d']:.6e}")
    print(f"  L_inf_d = {final_m['L_inf_d']:.6e}")

    if visualize:
        visualize_derivative(model, loss_hist, device)

    return model

# =========================
# 6) Visualization (derivative only)
# =========================
def visualize_derivative(model: nn.Module, loss_history: list, device: str = "cpu"):
    xs = torch.linspace(-1.0, 1.0, 2000, device=device).unsqueeze(1)
    xs.requires_grad_(True)
    yhat = model(xs)
    dhat = torch.autograd.grad(yhat.sum(), xs, create_graph=False)[0]
    dtrue = runge_dx(xs)

    xs_np    = xs.detach().cpu().numpy().squeeze(1)
    dhat_np  = dhat.detach().cpu().numpy().squeeze(1)
    dtrue_np = dtrue.detach().cpu().numpy().squeeze(1)

    # 導數對比
    plt.figure(figsize=(7,4))
    plt.plot(xs_np, dtrue_np, label="Runge f'(x)")
    plt.plot(xs_np, dhat_np,  label="NN f'(x)")
    plt.title("Derivative comparison")
    plt.xlabel("x"); plt.ylabel("derivative")
    plt.grid(True, alpha=0.3); plt.legend(); plt.show()

    # 導數絕對誤差
    derr_np = np.abs(dhat_np - dtrue_np)
    plt.figure(figsize=(7,4))
    plt.plot(xs_np, derr_np, label=r"|$\hat f'(x)-f'(x)$|")
    plt.title("Absolute derivative error across [-1,1]")
    plt.xlabel("x"); plt.ylabel("abs derivative error")
    plt.grid(True, alpha=0.3); plt.legend(); plt.yscale("log"); plt.show()

    # 訓練損失（導數 MSE）
    plt.figure(figsize=(7,4))
    plt.plot(np.arange(1, len(loss_history)+1), loss_history)
    plt.title("Training loss (derivative MSE)")
    plt.xlabel("step"); plt.ylabel("loss")
    plt.grid(True, alpha=0.3); plt.yscale("log"); plt.show()

# =========================
# 7) Main
# =========================
if __name__ == "__main__":
    CONFIG = {
        "steps": 2000,
        "batch_size": 512,
        "width": 64,
        "use_chebyshev": False,   # 改 True 可觀察端點更穩
        "lr": 1e-3,
        "weight_decay": 1e-6,
        "device": "cpu",
        "seed": 0,
        "verbose": True,
        "visualize": True,        # 改 True 顯示三張導數圖
    }
    _ = train(**CONFIG)